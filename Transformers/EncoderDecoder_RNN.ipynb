{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.2-cp310-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /Users/dikshapaliwal/virtualEnv/mlEnv/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: typing-extensions in /Users/dikshapaliwal/virtualEnv/mlEnv/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /Users/dikshapaliwal/virtualEnv/mlEnv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/dikshapaliwal/virtualEnv/mlEnv/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 mpmath-1.3.0 sympy-1.12 torch-2.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'SOS',\n",
       " 1: 'EOS',\n",
       " 2: 'you',\n",
       " 3: 'thank',\n",
       " 4: '?',\n",
       " 5: 'and',\n",
       " 6: 'how',\n",
       " 7: 'good,',\n",
       " 8: 'i',\n",
       " 9: 'am',\n",
       " 10: 'doing',\n",
       " 11: 'are'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOS_token = 0 #start of sentence token\n",
    "EOS_token = 1 #end of sentence token\n",
    "\n",
    "#index to word mapping\n",
    "index2words = {\n",
    "    SOS_token:'SOS',  #vocab dict\n",
    "    EOS_token:'EOS'\n",
    "\n",
    "}\n",
    "\n",
    "words  = \"How are you doing ? I am good, thank you and you ?\"\n",
    "word_list = set(words.lower().split(\" \")) #Get unique words from the above sentence\n",
    "\n",
    "\n",
    "for word in word_list:\n",
    "    index2words[len(index2words)] = word #because teh index of the latest word added will be the last position i.e. the now length of the vocab dict\n",
    "\n",
    "\n",
    "index2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SOS': 0,\n",
       " 'EOS': 1,\n",
       " 'you': 2,\n",
       " 'thank': 3,\n",
       " '?': 4,\n",
       " 'and': 5,\n",
       " 'how': 6,\n",
       " 'good,': 7,\n",
       " 'i': 8,\n",
       " 'am': 9,\n",
       " 'doing': 10,\n",
       " 'are': 11}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word to index mapping\n",
    "\n",
    "words2index = {word : key for key,word in index2words.items()}\n",
    "\n",
    "words2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6, 11,  2, 10,  2,  4]])\n"
     ]
    }
   ],
   "source": [
    "#to create torch tensors\n",
    "\n",
    "def convert2tensors(sentence):\n",
    "    word_list = sentence.lower().split(\" \")\n",
    "    sentence_index = [words2index[word] for word in word_list]\n",
    "\n",
    "    sentence_tensor = torch.tensor(sentence_index,dtype = torch.long).view(1,-1) #The `-1` in the second dimension means that PyTorch should automatically infer the size of this dimension based on the original size of the tensor. The idea is that PyTorch will calculate the size such that the total number of elements in the tensor remains the same.\n",
    "    return sentence_tensor\n",
    "    \n",
    "\n",
    "sent = \"How are you doing you ?\"\n",
    "print(convert2tensors(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size): \n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            input_size (_type_): size of the vocabulary dict\n",
    "            hidden_size (_type_): size of the vectors of the word embeddings/size of the vectors going into and coming out of the recurring units \n",
    "                                e.g. if an input vector or vocab word is represented as a vector of size(1,5) then the hidden size will also be (1,5)\n",
    "                                but the input size will be 12 if there are 12 words inthe vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size) #input_size = num of rows that the embedding wil have,hidden_size = no of elements that will be in each vector coming out of the embedding\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size,batch_first=True) #this GRU will be able to take vector of the size of the vector coming out of the embedding and will generate a vector of the same size\n",
    "        #batch_first=True -> the tensor that the GRU is gonna generate will have as a first dimension the batch\n",
    "\n",
    "    def forward(self,input_tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        output,hidden = self.gru(embedded) # we here only have one gru layer we can have as many as we like\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_size: torch.Size([1, 5, 3]) \n",
      " output: tensor([[[ 0.0650,  0.0368, -0.3729],\n",
      "         [ 0.5080, -0.2216, -0.2133],\n",
      "         [ 0.4138, -0.2536, -0.5126],\n",
      "         [ 0.6746, -0.4543, -0.2361],\n",
      "         [ 0.3519, -0.2177, -0.3059]]], grad_fn=<TransposeBackward1>)\n",
      "hidden_size: torch.Size([1, 1, 3]) \n",
      " hidden: tensor([[[ 0.3519, -0.2177, -0.3059]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Encoding a sentence\n",
    "\n",
    "encoder = EncoderRNN(len(words2index),hidden_size=3)\n",
    "\n",
    "sentence = \"How are you doing ?\"\n",
    "sent_tensor = convert2tensors(sentence)\n",
    "\n",
    "output,hidden = encoder(sent_tensor)\n",
    "\n",
    "print(f\"output_size: {output.size()} \\n output: {output}\") # 1 :batch size - coz only 1 sent,5: no of words in the sentence, 3:hidden size\"\n",
    "print(f\"hidden_size: {hidden.size()} \\n hidden: {hidden}\") #1: batch ,1: no of layers,3: hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_size: torch.Size([1, 5, 3]) \n",
      " output last element: tensor([[ 0.3519, -0.2177, -0.3059]], grad_fn=<SelectBackward0>)\n",
      "hidden_size: torch.Size([1, 1, 3]) \n",
      " hidden: tensor([[[ 0.3519, -0.2177, -0.3059]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"output_size: {output.size()} \\n output last element: {output[:,-1]}\")\n",
    "print(f\"hidden_size: {hidden.size()} \\n hidden: {hidden}\")\n",
    "\n",
    "#here both last element of output and the hidden is same co zwe only have 1 gru layer and its not bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            hidden_size (_type_): num \n",
    "            output_size (_type_): should be same as the size of the vocab list\n",
    "        \"\"\"\n",
    "        super(DecoderRNN,self).__init__()   #gonna initialize the nn.Module class\n",
    "        self.embedding = nn.Embedding(output_size,hidden_size) #diff embedding than that used in encoder, going to have as many rows as the output size\n",
    "        #embedding is going to generate a vector of size hidden size\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size,output_size)  #prediction head with linear layer to output the predictions\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,encoder_outputs,encoder_hidden):\n",
    "        batch_size = encoder_outputs.size(0) #this is the batch size of the encoder output\n",
    "        #we are goin to usde this batch size to generate the input to the deocder to  initialize teh process\n",
    "        decoder_input = torch.empty(batch_size,1,dtype= torch.long).fill_(SOS_token) #1 because we only need 1 input, batch_size because we need input for each of the element in the batch\n",
    "        #we fill it with the start of sentence token\n",
    "        decoder_hidden = encoder_hidden  #in the first step the hidden states are going to be inputted into teh GRu ar egoing to be the ones comin gout of encoder\n",
    "        decoder_outputs = []\n",
    "        \n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output,decoder_hidden = self.forward_step(decoder_input,decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            _,topIdx = decoder_output.topk(1)  #gives us the index of the word that has the max value in the decoder output ~ equivalent to argmax\n",
    "            decoder_input = topIdx.squeeze(-1).detach()  #squeee removes all the dimensions hthat have dimensionality 1, and we detach it to not confuse the gradient\n",
    "\n",
    "        decoder_outputs= torch.cat(decoder_outputs, dim = 1)  #conactenate decoder outputs to make a pytorch tensor\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs,dim =-1) #apply the log_softmax on deocdrr outputs\n",
    "\n",
    "        return decoder_outputs,decoder_hidden\n",
    "\n",
    "\n",
    "\n",
    "    def forward_step(self,input_tensor,hidden):  #we call it for each element in the output sequence, generates only one element in the output sequence\n",
    "        \"\"\"\n",
    "        We will iterate this forward step as many times as there are words in the input sentence\n",
    "        \"\"\"\n",
    "        output = self.embedding(input_tensor)\n",
    "        output = F.relu(output) #to add some non linearity to this NN\n",
    "        output,hidden = self.gru(output, hidden)  #hidden is the hidden states generated by the encoder\n",
    "        output = self.out(output)  # using the output from teh gru, we ar going to project it using the linear layer(i.e. self.out form init), to generate an output with a diff dimention\n",
    "\n",
    "\n",
    "        return output,hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 10\n",
    "VOCAB_SIZE = 1000\n",
    "OUTPUT_VECT_NUM = 10\n",
    "\n",
    "input_tensor = torch.rand(BATCH_SIZE,OUTPUT_VECT_NUM,HIDDEN_SIZE)  #hidden size= size of the tensor coming out of the decoder\n",
    "input_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 1000])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = nn.Linear(HIDDEN_SIZE,VOCAB_SIZE) #here what we were able to do is project the output from decoder\n",
    "\n",
    "out = linear_layer(input_tensor)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,indexes = out.topk(1)\n",
    "indexes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 1, 3, 1],\n",
       "        [3, 0, 0, 0],\n",
       "        [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another random vector\n",
    "\n",
    "out_ = torch.randn(3,4,5)  # has 5 vectors of size 4 and a total of 3 batches\n",
    "_,indexes_ = out_.topk(1)\n",
    "# indexes.size()\n",
    "# topk : gives out the top value index in the output for all inputs\n",
    "\n",
    "# indexes\n",
    "indexes_.squeeze() #just removes the dimensions that are 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0006, 0.0010, 0.0015,  ..., 0.0009, 0.0012, 0.0005],\n",
       "         [0.0005, 0.0011, 0.0011,  ..., 0.0010, 0.0008, 0.0006],\n",
       "         [0.0006, 0.0011, 0.0018,  ..., 0.0007, 0.0007, 0.0006],\n",
       "         ...,\n",
       "         [0.0006, 0.0011, 0.0013,  ..., 0.0010, 0.0010, 0.0008],\n",
       "         [0.0006, 0.0010, 0.0016,  ..., 0.0009, 0.0010, 0.0006],\n",
       "         [0.0007, 0.0010, 0.0014,  ..., 0.0011, 0.0008, 0.0006]],\n",
       "\n",
       "        [[0.0007, 0.0009, 0.0013,  ..., 0.0008, 0.0009, 0.0005],\n",
       "         [0.0005, 0.0010, 0.0013,  ..., 0.0009, 0.0008, 0.0006],\n",
       "         [0.0008, 0.0008, 0.0014,  ..., 0.0010, 0.0011, 0.0005],\n",
       "         ...,\n",
       "         [0.0008, 0.0009, 0.0014,  ..., 0.0010, 0.0008, 0.0006],\n",
       "         [0.0005, 0.0012, 0.0013,  ..., 0.0007, 0.0010, 0.0007],\n",
       "         [0.0006, 0.0011, 0.0012,  ..., 0.0008, 0.0008, 0.0006]],\n",
       "\n",
       "        [[0.0007, 0.0009, 0.0010,  ..., 0.0007, 0.0006, 0.0005],\n",
       "         [0.0006, 0.0010, 0.0016,  ..., 0.0008, 0.0009, 0.0005],\n",
       "         [0.0005, 0.0014, 0.0014,  ..., 0.0009, 0.0010, 0.0007],\n",
       "         ...,\n",
       "         [0.0006, 0.0011, 0.0015,  ..., 0.0006, 0.0011, 0.0005],\n",
       "         [0.0008, 0.0008, 0.0013,  ..., 0.0008, 0.0010, 0.0005],\n",
       "         [0.0008, 0.0008, 0.0012,  ..., 0.0010, 0.0008, 0.0004]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0007, 0.0008, 0.0014,  ..., 0.0011, 0.0007, 0.0006],\n",
       "         [0.0007, 0.0009, 0.0014,  ..., 0.0010, 0.0011, 0.0006],\n",
       "         [0.0007, 0.0010, 0.0010,  ..., 0.0009, 0.0010, 0.0005],\n",
       "         ...,\n",
       "         [0.0005, 0.0010, 0.0009,  ..., 0.0009, 0.0008, 0.0005],\n",
       "         [0.0007, 0.0010, 0.0012,  ..., 0.0008, 0.0011, 0.0005],\n",
       "         [0.0005, 0.0013, 0.0013,  ..., 0.0009, 0.0009, 0.0010]],\n",
       "\n",
       "        [[0.0005, 0.0012, 0.0015,  ..., 0.0008, 0.0009, 0.0006],\n",
       "         [0.0009, 0.0007, 0.0012,  ..., 0.0011, 0.0009, 0.0004],\n",
       "         [0.0007, 0.0012, 0.0013,  ..., 0.0008, 0.0010, 0.0005],\n",
       "         ...,\n",
       "         [0.0007, 0.0008, 0.0014,  ..., 0.0011, 0.0008, 0.0006],\n",
       "         [0.0009, 0.0008, 0.0016,  ..., 0.0010, 0.0010, 0.0004],\n",
       "         [0.0007, 0.0010, 0.0016,  ..., 0.0011, 0.0010, 0.0006]],\n",
       "\n",
       "        [[0.0007, 0.0008, 0.0013,  ..., 0.0009, 0.0008, 0.0003],\n",
       "         [0.0005, 0.0010, 0.0014,  ..., 0.0010, 0.0007, 0.0005],\n",
       "         [0.0006, 0.0009, 0.0018,  ..., 0.0010, 0.0009, 0.0006],\n",
       "         ...,\n",
       "         [0.0006, 0.0012, 0.0014,  ..., 0.0008, 0.0010, 0.0007],\n",
       "         [0.0006, 0.0011, 0.0016,  ..., 0.0009, 0.0010, 0.0007],\n",
       "         [0.0008, 0.0010, 0.0011,  ..., 0.0009, 0.0010, 0.0006]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "F.softmax(out,dim= -1) #applying softmax on out, dim =-1 normalize on last dimension\n",
    "# .sum(-1) if we do this we get a tensor of all 1s\n",
    "#i.e. teh softmax worked since the numbers ar enow in range 0-1\n",
    "# and in the last dimension sum to 1 like we wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9193, -1.0916, -0.8731,  1.6631, -0.0410],\n",
       "         [-0.6673,  0.3435,  0.1451,  0.2436,  0.1969],\n",
       "         [-0.3215, -0.0290, -0.5229,  0.7086, -0.2852],\n",
       "         [ 0.5290,  1.4926, -0.9996, -0.3783, -1.1827]],\n",
       "\n",
       "        [[ 0.5104,  0.8643, -1.3237,  1.5819,  0.2093],\n",
       "         [ 0.5146, -0.2411, -0.2051, -1.0754,  0.2297],\n",
       "         [ 1.7449, -0.4838,  0.9930, -1.0516,  0.4883],\n",
       "         [ 1.4940,  0.6859, -0.3067,  0.9819, -0.2958]],\n",
       "\n",
       "        [[-0.1065, -0.3531,  1.2323,  0.9363, -0.3118],\n",
       "         [-1.6017, -0.0606,  0.4528, -0.0395,  0.2500],\n",
       "         [-1.5264, -0.8603,  1.6205,  0.8130,  0.6255],\n",
       "         [ 0.1485, -0.6808,  0.3740,  0.0938,  0.1565]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3317, -3.3425, -3.1240, -0.5878, -2.2919],\n",
       "         [-2.3851, -1.3743, -1.5727, -1.4743, -1.5210],\n",
       "         [-1.9453, -1.6528, -2.1467, -0.9152, -1.9090],\n",
       "         [-1.4866, -0.5230, -3.0152, -2.3939, -3.1983]],\n",
       "\n",
       "        [[-1.8316, -1.4778, -3.6658, -0.7601, -2.1327],\n",
       "         [-1.0691, -1.8247, -1.7887, -2.6590, -1.3540],\n",
       "         [-0.6548, -2.8835, -1.4067, -3.4513, -1.9114],\n",
       "         [-0.8659, -1.6740, -2.6666, -1.3780, -2.6557]],\n",
       "\n",
       "        [[-2.2243, -2.4709, -0.8856, -1.1815, -2.4296],\n",
       "         [-3.2033, -1.6621, -1.1488, -1.6411, -1.3516],\n",
       "         [-3.8108, -3.1447, -0.6639, -1.4714, -1.6589],\n",
       "         [-1.5356, -2.3649, -1.3101, -1.5903, -1.5276]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(out_,dim = -1)\n",
    "\n",
    "#we usually use log for classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the Encoder - Decoder together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6, 11,  2, 10,  4]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderRNN(hidden_size = 3,output_size=len(words2index))\n",
    "encoder = EncoderRNN(len(words2index),hidden_size=3)\n",
    "\n",
    "sentence = \"How are you doing ?\"\n",
    "\n",
    "input_tensor =convert2tensors(sentence)\n",
    "output, hidden = encoder(input_tensor)\n",
    "\n",
    "decoder_outputs , decoder_hiddens = decoder(output,hidden)\n",
    "decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fucntion that takes a tensor and generates a sentence\n",
    "\n",
    "def convert2sentence(tensor):\n",
    "    words_list = [index2words[idx.item()] for idx in tensor] #idx.item will get the python value at that index in the tensor\n",
    "    return \" \".join(words_list)\n",
    "\n",
    "_,topIdx = decoder_outputs.topk(1)\n",
    "decoded_ids = topIdx.squeeze()\n",
    "decoded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you you you you you you you you you you'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert2sentence(decoded_ids)  #this output is bad because we didnt train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('mlEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c2a6a9e04fdc35e60332e4195b8e06175fffd3af56722b541c493fbbe1c5eab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
